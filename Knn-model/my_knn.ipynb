{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "part1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlytg55qxU2i"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "from scipy.spatial import distance\n",
        "import statistics\n",
        "import random"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-V4_B8HzxcWZ"
      },
      "source": [
        "# New section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8hviZeoqnAJ",
        "outputId": "0fa0a282-1a31-454d-cfdc-698a81f00b41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87cM15MuxU2p"
      },
      "source": [
        "#reading and storing train data\n",
        "train = pd.read_csv(\"train.csv\")  \n",
        "#reading and storing test data\n",
        "test = pd.read_csv(\"test.csv\")\n",
        "##reading and storing stop words\n",
        "f = open('stop_words.txt', 'r+')  \n",
        "stop_words = f.read().splitlines()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCHlTz0uxU27"
      },
      "source": [
        "def word_split(sent): #A function to split a string into a list of words\n",
        "    words = re.sub(\"[\\W]\",\" \", sent).split()\n",
        "    return words\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYzWzfbYROqg"
      },
      "source": [
        "def clean_data(x):\n",
        "  #converting to lowercase\n",
        "  x = x.apply(lambda x: x.astype(str).str.lower())\n",
        "  #removing stop words\n",
        "  for i in stop_words : \n",
        "      x = x.replace(to_replace=r'\\b%s\\b'%i, value=\"\",regex=True)\n",
        "  #removing punctuations\n",
        "  table = str.maketrans(dict.fromkeys(string.punctuation))\n",
        "  for index, value in x['Tweet'].items():\n",
        "      x['Tweet'][index]=x['Tweet'][index].translate(table)\n",
        "  #removing numbers\n",
        "  x = x.replace(to_replace=r'\\d', value=\"\",regex=True)\n",
        "  return x"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzkYe2gJc-D8"
      },
      "source": [
        "def knn(k):\n",
        "  p=0\n",
        "  s=distances[0].size\n",
        "  print(\"k = \", k)\n",
        "  perdicted_label = []\n",
        "  while p<s:\n",
        "    t=k\n",
        "    dist = sorted(distances.iloc[p])\n",
        "    #print(minimum_dist)\n",
        "    while t>0:\n",
        "      #print(k,\"   \",t)\n",
        "     ##print(\"KK=\",k)\n",
        "      index_distance = []\n",
        "      n=0\n",
        "      for x, y in enumerate(distances.iloc[p]):\n",
        "        if n==t:\n",
        "          #print(n)\n",
        "          break\n",
        "        if dist[n]==y:\n",
        "          index_distance.append(x)\n",
        "          n+=1\n",
        "      #print(labels[index_distance])\n",
        "      try:\n",
        "        xx = statistics.mode(labels[index_distance])\n",
        "      except:\n",
        "        #print(index_distance)\n",
        "        #print(\"Multiple modes for k = \", t)\n",
        "        t-=1\n",
        "        continue\n",
        "      else:\n",
        "        #print(xx)\n",
        "        perdicted_label.append(xx)\n",
        "        #print(perdicted_label)\n",
        "        p+=1\n",
        "        break\n",
        "  return perdicted_label"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIef0wTUxiVP"
      },
      "source": [
        "def measures(perdicted_label,accuracy,precision,f1,recall):\n",
        "  #Accuracy Calculation\n",
        "  correct = 0\n",
        "  for x,y in enumerate(perdicted_label):\n",
        "    if y == test['Sentiment'][x]:\n",
        "      correct+= 1\n",
        "  Accu = (correct/test['Sentiment'].size)\n",
        "  accuracy.append(Accu)\n",
        "  print(\"Accuracy: \", Accu)\n",
        "  #calculating false and true positives, negatives, and neutrals\n",
        "  p_pos=0\n",
        "  p_neg=0\n",
        "  p_nut=0\n",
        "  n_pos=0\n",
        "  n_neg=0\n",
        "  n_nut=0\n",
        "  nu_pos=0\n",
        "  nu_neg=0\n",
        "  nu_nut=0\n",
        "  for x,y in enumerate(perdicted_label):\n",
        "    if   (y == \"positive\") & (test['Sentiment'][x] == \"positive\"):  p_pos+=1\n",
        "    elif (y == \"positive\") & (test['Sentiment'][x] == \"negative\"):  p_neg+=1\n",
        "    elif (y == \"positive\") & (test['Sentiment'][x] == \"neutral\"):   p_nut+=1\n",
        "    elif (y == \"negative\") & (test['Sentiment'][x] == \"negative\"):  n_neg+=1\n",
        "    elif (y == \"negative\") & (test['Sentiment'][x] == \"positive\"):  n_pos+=1\n",
        "    elif (y == \"negative\") & (test['Sentiment'][x] == \"neutral\"):   n_nut+=1\n",
        "    elif (y == \"neutral\") & (test['Sentiment'][x] == \"positive\"):   nu_pos+=1\n",
        "    elif (y == \"neutral\") & (test['Sentiment'][x] == \"negative\"):   nu_neg+=1\n",
        "    elif (y == \"neutral\") & (test['Sentiment'][x] == \"neutral\"):    nu_nut+=1\n",
        "  #calculating macroaverage recall\n",
        "  pos_recall= p_pos/(p_pos+p_neg+p_nut)\n",
        "  neg_recall= n_neg/(n_pos+n_neg+n_nut)\n",
        "  nut_recall= nu_nut/(nu_pos+nu_neg+nu_nut)\n",
        "  macro_avg_recall = (pos_recall+neg_recall+nut_recall)/3\n",
        "  recall.append(macro_avg_recall)\n",
        "  #calculating macroaverage precision\n",
        "  pos_precision= p_pos/(p_neg+p_pos+p_nut)\n",
        "  neg_precision= n_neg/(n_neg+n_pos+n_nut)\n",
        "  nut_precision= nu_nut/(nu_neg+nu_pos+nu_nut)\n",
        "  macro_avg_precision = (pos_precision+neg_precision+nut_precision)/3\n",
        "  precision.append(macro_avg_precision)\n",
        "  #calculating macroaverage F1-score\n",
        "  F1_score = (2*macro_avg_precision*macro_avg_recall)/(macro_avg_recall+macro_avg_precision)\n",
        "  f1.append(F1_score)\n",
        "  #outputing macroaverage recall precision and F1-Score\n",
        "  print(\"Macroaverage Recall: \", macro_avg_recall)\n",
        "  print(\"Macroaverage Percision: \", macro_avg_precision)\n",
        "  print(\"F1 Score: \", F1_score)\n",
        "  #Building a confusion matrix\n",
        "  print(\"Confusion Matrix: \")\n",
        "  conf = {'Outputs/Gold Labels' : ['positive','neutral','negative'], 'positive' : [p_pos,p_nut,p_neg], 'neutral' : [nu_pos,nu_nut,nu_neg], 'negative' : [n_pos,n_nut,n_neg]}\n",
        "  confusion=pd.DataFrame(conf, columns= ['Outputs/Gold Labels', 'positive', 'neutral', 'negative'])\n",
        "  print(confusion)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQ97VTpsRQXF"
      },
      "source": [
        "train = clean_data(train)\n",
        "test = clean_data(test)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "QEfloSPPxU2u"
      },
      "source": [
        "#converting to lowercase\n",
        "train = train.apply(lambda x: x.astype(str).str.lower())\n",
        "#removing stop words\n",
        "for i in stop_words : \n",
        "    train = train.replace(to_replace=r'\\b%s\\b'%i, value=\"\",regex=True)\n",
        "#removing punctuations\n",
        "table = str.maketrans(dict.fromkeys(string.punctuation))\n",
        "for index, value in train['Tweet'].items():\n",
        "    train['Tweet'][index]=train['Tweet'][index].translate(table)\n",
        "#removing numbers\n",
        "train = train.replace(to_replace=r'\\d', value=\"\",regex=True)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYzqW8m3KRSB"
      },
      "source": [
        "\n",
        "#converting to lowercase\n",
        "test = test.apply(lambda x: x.astype(str).str.lower())\n",
        "#removing stop words\n",
        "for i in stop_words : \n",
        "    test = test.replace(to_replace=r'\\b%s\\b'%i, value=\"\",regex=True)\n",
        "#removing punctuations\n",
        "table = str.maketrans(dict.fromkeys(string.punctuation))\n",
        "for index, value in test['Tweet'].items():\n",
        "    test['Tweet'][index]=test['Tweet'][index].translate(table)\n",
        "#removing numbers\n",
        "test = test.replace(to_replace=r'\\d', value=\"\",regex=True)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPgIQZQvy-Zz"
      },
      "source": [
        "vocabulary= [] #building vocabulary series for bag of words from train data\n",
        "for x in train['Tweet'].tolist():\n",
        "    a = word_split(x)\n",
        "    vocabulary.extend(a);\n",
        "vocabulary = list(set(vocabulary))\n",
        "vocab=pd.Series(vocabulary)\n",
        "dup_vocab = vocab"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lg6deGGVxU3I"
      },
      "source": [
        "#building bag of words with vocabulary as columns and tweets as rows from train data\n",
        "bow = pd.DataFrame (columns=dup_vocab)\n",
        "ss = len(dup_vocab)\n",
        "for x in train['Tweet']:\n",
        "    c = word_split(x)\n",
        "    bow_vector = np.zeros(ss)\n",
        "    for d in c:\n",
        "        for i, y in enumerate(dup_vocab):\n",
        "            if y==d:\n",
        "                bow_vector[i] = bow_vector[i] + 1\n",
        "    #a = pd.DataFrame([bow_vector],columns = dup_vocab)\n",
        "    bow = bow.append(pd.Series(bow_vector, index=dup_vocab),ignore_index=True)\n",
        "    #print(bow.shape)\n",
        "dup_bow = bow"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbIQS-4AgM74"
      },
      "source": [
        "distances= []\n",
        "#building bag of words with vocabulary as columns and tweets as rows from test data\n",
        "test_bow = pd.DataFrame (columns=vocab)\n",
        "for x in test['Tweet'].tolist():\n",
        "    c = word_split(x)\n",
        "    test_bow_vector = np.zeros(ss)\n",
        "    for d in c:\n",
        "      for i, y in enumerate(vocab):\n",
        "        if y==d:\n",
        "          test_bow_vector[i] = test_bow_vector[i] + 1\n",
        "    #a = pd.DataFrame([test_bow_vector],columns = dup_vocab)\n",
        "    test_bow = test_bow.append(pd.Series(test_bow_vector, index=vocab),ignore_index=True)\n",
        "    #print(test_bow.shape)\n",
        "    #print(test_bow)\n",
        "test_dup_bow = test_bow"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60Ac1Uekpl0R"
      },
      "source": [
        "#finding euclidean distances\n",
        "distances = scipy.spatial.distance.cdist(test_dup_bow.values, dup_bow.values, metric='euclidean')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4NS2_q4rV1t"
      },
      "source": [
        "#distances as a dataframe\n",
        "distances = pd.DataFrame(distances)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdm70mI8vEvP"
      },
      "source": [
        "labels=train['Sentiment'] #train data labels (Gold labels)\n",
        "accuracy = []\n",
        "percision = []\n",
        "f1 = []\n",
        "recall= []\n",
        "p_label = knn(10) #predicting labels for test data for k=10\n",
        "measures(p_label,accuracy,percision,f1,recall) #measuring accuracy, confusion matrix, precision, f1-score, and recall \n",
        "p_label = knn(7) #predicting labels for test data for k=7\n",
        "measures(p_label,accuracy,percision,f1,recall) #measuring accuracy, confusion matrix, precision, f1-score, and recall \n",
        "p_label = knn(5) #predicting labels for test data for k=5\n",
        "measures(p_label,accuracy,percision,f1,recall) #measuring accuracy, confusion matrix, precision, f1-score, and recall \n",
        "p_label = knn(3) #predicting labels for test data for k=3\n",
        "measures(p_label,accuracy,percision,f1,recall) #measuring accuracy, confusion matrix, precision, f1-score, and recall \n",
        "p_label = knn(1) #predicting labels for test data for k=\n",
        "measures(p_label,accuracy,percision,f1,recall) #measuring accuracy, confusion matrix, precision, f1-score, and recall "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UruYTU3YJh9L"
      },
      "source": [
        "k= [10,7,5,3,1]\n",
        "#Plotting graphs for accuracy, precision, recall, and f1-score against all k values\n",
        "fig, axs = plt.subplots(2,2)\n",
        "axs[0,0].plot(k,accuracy)\n",
        "axs[0,0].set_title('Accuracy')\n",
        "axs[1,0].plot(k,percision)\n",
        "axs[1,0].set_title('Precision')\n",
        "axs[1,1].plot(k,recall)\n",
        "axs[1,1].set_title('Recall')\n",
        "axs[0,1].plot(k,f1)\n",
        "axs[0,1].set_title('F1 Score')\n",
        "for ax in axs.flat:\n",
        "    ax.set(xlabel='k-values')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}